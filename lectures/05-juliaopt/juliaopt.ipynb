{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Optimization-in-Julia\" data-toc-modified-id=\"Optimization-in-Julia-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Optimization in Julia</a></div><div class=\"lev2 toc-item\"><a href=\"#Flowchart\" data-toc-modified-id=\"Flowchart-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Flowchart</a></div><div class=\"lev2 toc-item\"><a href=\"#DCP-Using-Convex.jl\" data-toc-modified-id=\"DCP-Using-Convex.jl-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>DCP Using Convex.jl</a></div><div class=\"lev3 toc-item\"><a href=\"#Example:-microbiome-regression-analysis\" data-toc-modified-id=\"Example:-microbiome-regression-analysis-121\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Example: microbiome regression analysis</a></div><div class=\"lev3 toc-item\"><a href=\"#Sum-to-zero-regression\" data-toc-modified-id=\"Sum-to-zero-regression-122\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Sum-to-zero regression</a></div><div class=\"lev3 toc-item\"><a href=\"#Sum-to-zero-lasso\" data-toc-modified-id=\"Sum-to-zero-lasso-123\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Sum-to-zero lasso</a></div><div class=\"lev3 toc-item\"><a href=\"#Sum-to-zero-group-lasso\" data-toc-modified-id=\"Sum-to-zero-group-lasso-124\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>Sum-to-zero group lasso</a></div><div class=\"lev3 toc-item\"><a href=\"#Example:-matrix-completion\" data-toc-modified-id=\"Example:-matrix-completion-125\"><span class=\"toc-item-num\">1.2.5&nbsp;&nbsp;</span>Example: matrix completion</a></div><div class=\"lev2 toc-item\"><a href=\"#Nonlinear-programming-(NLP)\" data-toc-modified-id=\"Nonlinear-programming-(NLP)-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Nonlinear programming (NLP)</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization in Julia\n",
    "\n",
    "This lecture gives an overview of some optimization tools in Julia.\n",
    "\n",
    "**Warning**: Some code chunks will not run on the server because Mosek/Gurobi (commercial software) are not available on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flowchart\n",
    "\n",
    "* Statisticians do optimizations in daily life: maximum likelihood estimation, machine learning, ...\n",
    "\n",
    "* Category of optimization problems:\n",
    "\n",
    "    1. Problems with analytical solutions: least squares, principle component analysis, canonical correlation analysis, ...\n",
    "    \n",
    "    2. Problems subject to Disciplined Convex Programming (DCP): linear programming (LP), quadratic programming (QP), second-order cone programming (SOCP), semidefinite programming (SDP), and geometric programming (GP).\n",
    "    \n",
    "    3. Nonlinear programming (NLP): Newton type algorithms, Fisher scoring algorithm, EM algorithm, MM algorithms. \n",
    "    \n",
    "    4. Large scale optimization: ADMM, SGD, ...\n",
    "    \n",
    "![Flowchart](./optimization_flowchart.png)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCP Using Convex.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard convex problem classes like LP (linear programming), QP (quadratic programming), SOCP (second-order cone programming), SDP (semidefinite programming), and GP (geometric programming), are becoming a **technology**.\n",
    "\n",
    "![DCP Hierarchy](./convex-hierarchy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting familiar with **good** optimization softwares broadens the scope and scale of problems we are able to solve in statistics. Following table lists some of the best optimization softwares. \n",
    "\n",
    "|           |   | LP | MILP | SOCP |     MISOCP     | SDP | GP | NLP | MINLP |   | R | Matlab | Julia | Python |   | Cost |\n",
    "|:---------:|:-:|:--:|:----:|:----:|:--------------:|:---:|:--:|:---:|:-----:|:-:|:-:|:------:|:-----:|:------:|:-:|:----:|\n",
    "|   **modeling tools**   |   |    |      |      |                |     |    |     |       |   |   |        |       |        |   |      |\n",
    "|    cvx    |   |  x |   x  |   x  |        x       |  x  |  x |     |       |   |   |    x   |       |    x   |   |   A  |\n",
    "| Convex.jl |   |  x |   x  |   x  |        x       |  x  |    |     |       |   |   |        |   x   |        |   |   O  |\n",
    "|  JuMP.jl  |   |  x |   x  |   x  |        x       |     |    |  x  |   x   |   |   |        |   x   |        |   |   O  |\n",
    "|   **convex solvers** |   |    |      |      |                |     |    |     |       |   |   |        |       |        |   |      |\n",
    "|   Mosek   |   |  x |   x  |   x  |        x       |  x  |  x |  x  |       |   | x |    x   |   x   |    x   |   |   A  |\n",
    "|   Gurobi  |   |  x |   x  |   x  |        x       |     |    |     |       |   | x |    x   |   x   |    x   |   |   A  |\n",
    "|   CPLEX   |   |  x |   x  |   x  |        x       |     |    |     |       |   | x |    x   |   x   |    x   |   |   A  |\n",
    "|    SCS    |   |  x |      |   x  |                |  x  |    |     |       |   |   |    x   |   x   |    x   |   |   O  |\n",
    "|   SeDuMi  |   |  x |      |   x  |                |  x  |  ? |     |       |   |   |    x   |       |        |   |   O  |\n",
    "|   SDPT3   |   |  x |      |   x  |                |  x  |  ? |     |       |   |   |    x   |       |        |   |   O  |\n",
    "|   **NLP solvers**  |   |    |      |      |                |     |    |     |       |   |   |        |       |        |   |      |\n",
    "|   NLopt   |   |  x |      |      |                |     |    |  x  |       |   |   |    x   |   x   |    x   |   |   O  |\n",
    "|   Ipopt   |   |  x |      |      |                |     |    |  x  |       |   |   |    x   |   x   |    x   |   |   O  |\n",
    "|   KNITRO  |   |  x |   x  |      |                |     |    |  x  |   x   |   | x |    x   |   x   |    x   |   |   $  |\n",
    "\n",
    "* O: open source  \n",
    "* A: free academic license  \n",
    "* $: commercial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Difference between **modeling tool** and **solvers**\n",
    "\n",
    "    - **Modeling tools** such as cvx (for Matlab) and Convex.jl (Julia analog of cvx) implement the disciplined convex programming (DCP) paradigm proposed by Grant and Boyd (2008) <http://stanford.edu/~boyd/papers/disc_cvx_prog.html>. DCP prescribes a set of simple rules from which users can construct convex optimization problems easily.\n",
    "    \n",
    "    - **Solvers** (Mosek, Gurobi, Cplex, SCS, ...) are concrete software implementation of optimization algorithms. My favorite ones are: Mosek/Gurobi/SCS for DCP and Ipopt/NLopt for nonlinear programming. Mosek and Gurobi are commercial software but free for academic use. SCS/Ipopt/NLopt are open source.  \n",
    "    \n",
    "    - Modeling tools usually have the capability to use a variety of solvers. But modeling tools are solver agnostic so users do not have to worry about specific solver interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example: microbiome regression analysis\n",
    "\n",
    "We illustrate optimization tools in Julia using microbiome analysis as an example.\n",
    "\n",
    "16S microbiome sequencing techonology generates sequence counts of various organisms (OTUs, operational taxonomic units) in samples. \n",
    "\n",
    "![Microbiome Data](./microbiome_data.png)\n",
    "\n",
    "For statistical analysis, counts are normalized into **proportions** for each sample, resulting in a covariate matrix $\\mathbf{X}$ with all rows summing to 1. For identifiability, we need to add a sum-to-zero constraint to the regression cofficients. In other words, we need to solve a **constrained least squares problem**  \n",
    "$$\n",
    "    \\text{minimize} \\frac{1}{2} \\|\\mathbf{y} - \\mathbf{X} \\beta\\|_2^2\n",
    "$$\n",
    "subject to the constraint $\\sum_{j=1}^p \\beta_j = 0$. For simplicity we ignore intercept and non-OTU covariates in this presentation.\n",
    "\n",
    "Let's first generate an artifical data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "srand(123) # seed\n",
    "\n",
    "n, p = 100, 50\n",
    "X = rand(n, p)\n",
    "X = Diagonal(1 ./ vec(sum(X, 2))) * X\n",
    "β = sprandn(p, 0.1) # sparse vector with about 10% non-zero entries\n",
    "y = X * β + randn(n);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum-to-zero regression\n",
    "\n",
    "The sum-to-zero contrained least squares is a standard quadratic programming (QP) problem so should be solved easily by any QP solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Convex.jl package and chooose a solver. Note Gurobi and Mosek (commercial) are not installed on the server; use SCS solver (open source) on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Convex\n",
    "\n",
    "## Use Gurobi solver\n",
    "#using Gurobi\n",
    "#solver = GurobiSolver(OutputFlag=0)\n",
    "#set_default_solver(solver)\n",
    "\n",
    "# # Use Mosek solver\n",
    "# using Mosek\n",
    "# solver = MosekSolver(LOG=1)\n",
    "# set_default_solver(solver)\n",
    "\n",
    "# Use SCS solver\n",
    "using SCS\n",
    "solver = SCSSolver(verbose=1)\n",
    "set_default_solver(solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "β̂cls = Variable(size(X, 2))\n",
    "problem = minimize(0.5sumsquares(y - X * β̂cls)) # objective\n",
    "problem.constraints += sum(β̂cls) == 0 # constraint\n",
    "@time solve!(problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum-to-zero lasso\n",
    "\n",
    "Suppose we want to know which organisms (OTU) are associated with the response. We can answer this question using a sum-to-zero contrained lasso\n",
    "$$\n",
    "    \\text{minimize} \\frac 12 \\|\\mathbf{y} - \\mathbf{X} \\beta\\|_2^2 + \\lambda \\|\\beta\\|_1\n",
    "$$\n",
    "subject to the constraint $\\sum_{j=1}^p \\beta_j = 0$. Varying $\\lambda$ from small to large values will generate a solution path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SCS solver\n",
    "using SCS\n",
    "solver = SCSSolver(verbose=0)\n",
    "set_default_solver(solver)\n",
    "\n",
    "# solve at a grid of λ\n",
    "λgrid = 0:0.01:0.35\n",
    "β̂path = zeros(length(λgrid), size(X, 2)) # each row is β̂ at a λ\n",
    "β̂classo = Variable(size(X, 2))\n",
    "@time for i in 1:length(λgrid)\n",
    "    λ = λgrid[i]\n",
    "    # objective\n",
    "    problem = minimize(0.5sumsquares(y - X * β̂classo) + λ * sum(abs, β̂classo))\n",
    "    # constraint\n",
    "    problem.constraints += sum(β̂classo) == 0 # constraint\n",
    "    solve!(problem)\n",
    "    β̂path[i, :] = β̂classo.value\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots; gr()\n",
    "using LaTeXStrings\n",
    "\n",
    "p = plot(collect(λgrid), β̂path, legend=:none)\n",
    "xlabel!(p, L\"\\lambda\")\n",
    "ylabel!(p, L\"\\hat \\beta\")\n",
    "title!(p, \"Sum-to-Zero Lasso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum-to-zero group lasso\n",
    "\n",
    "Suppose we want to do variable selection not at the OTU level, but at the Phylum level. OTUs are clustered into various Phyla. We can answer this question using a sum-to-zero contrained group lasso\n",
    "$$\n",
    "    \\text{minimize} \\frac 12 \\|\\mathbf{y} - \\mathbf{X} \\beta\\|_2^2 + \\lambda \\sum_j \\|\\mathbf{\\beta}_j\\|_2\n",
    "$$\n",
    "subject to the constraint $\\sum_{j=1}^p \\beta_j = 0$, where $\\mathbf{\\beta}_j$ are regression coefficients corresponding to the $j$-th phylum. This is a second-order cone programming (SOCP) problem readily modeled by Convex.jl.\n",
    "\n",
    "Let's assume each 10 continuous OTUs belong to one Phylum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SCS solver\n",
    "using SCS\n",
    "solver = SCSSolver(verbose=0)\n",
    "set_default_solver(solver)\n",
    "\n",
    "# solve at a grid of λ\n",
    "λgrid = 0.1:0.005:0.5\n",
    "β̂pathgrp = zeros(length(λgrid), size(X, 2)) # each row is β̂ at a λ\n",
    "β̂classo = Variable(size(X, 2))\n",
    "@time for i in 1:length(λgrid)\n",
    "    λ = λgrid[i]\n",
    "    # objective\n",
    "    obj = 0.5sumsquares(y - X * β̂classo)\n",
    "    for j in 1:(size(X, 2)/10)\n",
    "        βj = β̂classo[(10(j-1)+1):10j]\n",
    "        obj += λ * vecnorm(βj)\n",
    "    end\n",
    "    problem = minimize(obj)\n",
    "    # constraint\n",
    "    problem.constraints += sum(β̂classo) == 0 # constraint\n",
    "    solve!(problem)\n",
    "    β̂pathgrp[i, :] = β̂classo.value\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = plot(collect(λgrid), β̂pathgrp, legend=:none)\n",
    "xlabel!(p2, L\"\\lambda\")\n",
    "ylabel!(p2, L\"\\hat \\beta\")\n",
    "title!(p2, \"Sum-to-Zero Group Lasso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: matrix completion\n",
    "\n",
    "Load the $128 \\times 128$ Lena picture with missing pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Images\n",
    "\n",
    "lena = load(\"lena128missing.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to real matrices\n",
    "Y = Float64.(lena)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill out the missin pixels uisng a **matrix completion** technique developed by Candes and Tao\n",
    "$$\n",
    "    \\text{minimize } \\|\\mathbf{X}\\|_*\n",
    "$$\n",
    "$$\n",
    "    \\text{subject to } x_{ij} = y_{ij} \\text{ for all observed entries } (i, j).\n",
    "$$\n",
    "Here $\\|\\mathbf{M}\\|_* = \\sum_i \\sigma_i(\\mathbf{M})$ is the nuclear norm. In words we seek the matrix with minimal nuclear norm that agrees with the observed entries. This is a semidefinite programming (SDP) problem readily modeled by Convex.jl.\n",
    "\n",
    "This example takes long because of high dimensionality. **Don't run on server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Mosek solver\n",
    "using Mosek\n",
    "solver = MosekSolver(LOG=1)\n",
    "set_default_solver(solver)\n",
    "\n",
    "# Linear indices of obs. entries\n",
    "obsidx = find(Y .≠ 0.0)\n",
    "# Create optimization variables\n",
    "X = Convex.Variable(size(Y))\n",
    "# Set up optmization problem\n",
    "problem = minimize(nuclearnorm(X))\n",
    "problem.constraints += X[obsidx] == Y[obsidx]\n",
    "# Solve the problem by calling solve\n",
    "@time solve!(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorview(Gray, X.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear programming (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We use MLE of Gamma distribution to illustrate some rudiments of nonlinear programming (NLP) in Julia. \n",
    "\n",
    "Let $x_1,\\ldots,x_m$ be a random sample from the gamma density\n",
    "$$\n",
    "f(x) = \\Gamma(\\alpha)^{-1} \\beta^{\\alpha} x^{\\alpha-1} e^{-\\beta x}\n",
    "$$\n",
    "on $(0,\\infty)$. The loglikelihood function is\n",
    "$$\n",
    "    L(\\alpha, \\beta) = m [- \\ln \\Gamma(\\alpha) + \\alpha \\ln \\beta + (\\alpha - 1)\\overline{\\ln x} - \\beta \\bar x],\n",
    "$$\n",
    "where $\\overline{x} = \\frac{1}{m} \\sum_{i=1}^m x_i$ and \n",
    "$\\overline{\\ln x} = \\frac{1}{m} \\sum_{i=1}^m \\ln x_i$. Following function \n",
    "evaluates the log-pdf of one data point `x` at parameter `α` and `β`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gamma_logpdf(x::Vector, α::Real, β::Real)\n",
    "    m = length(x)\n",
    "    avg = mean(x)\n",
    "    logavg = sum(log, x) / m\n",
    "    m * (- lgamma(α) + α * log(β) + (α - 1) * logavg - β * avg)\n",
    "end\n",
    "x = rand(5)\n",
    "gamma_logpdf(x, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many optimization algorithms involve taking derivatives of the objective function. The `ForwardDiff` package implements automatic differentiation. For example, to compute the derivative and Hessian of the log-likelihood with data `x` at `α=1.0` and `β=1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "ForwardDiff.gradient(r -> gamma_logpdf(x, r...), [1.0; 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.hessian(r -> gamma_logpdf(x, r...), [1.0; 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions\n",
    "\n",
    "srand(123)\n",
    "(n, p) = (1000, 2)\n",
    "(α, β) = 5.0 * rand(p)\n",
    "x = rand(Gamma(α, β), n)\n",
    "println(\"True parameter values:\")\n",
    "println(\"α = \", α, \", β = \", β)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use JuMP.jl to define and solve our NLP problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Ipopt, NLopt\n",
    "\n",
    "m = Model(solver = IpoptSolver())\n",
    "# m = Model(solver = NLoptSolver(algorithm=:LD_MMA))\n",
    "\n",
    "myf(a, b) = gamma_logpdf(x, a, b)\n",
    "JuMP.register(m, :myf, 2, myf, autodiff=true)\n",
    "@variable(m, α >= 1e-8)\n",
    "@variable(m, β >= 1e-8)\n",
    "@NLobjective(m, Max, myf(α, β))\n",
    "\n",
    "print(m)\n",
    "status = solve(m)\n",
    "\n",
    "println(\"MLE (JuMP):\")\n",
    "println(\"α = \", α, \", β = \", β)\n",
    "println(\"Objective value: \", getobjectivevalue(m))\n",
    "println(\"α = \", getvalue(α), \", β = \", 1 / getvalue(β))\n",
    "println(\"MLE (Distribution package):\")\n",
    "println(fit_mle(Gamma, x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.4",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "153px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
